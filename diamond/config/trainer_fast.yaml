defaults:
  - _self_
  - env: csgo
  - agent: csgo
  - world_model_env: fast

hydra:
  job:
    chdir: True

wandb:
  mode: disabled
  project: null
  entity: null
  name: "fast_training"
  group: null
  tags: null
  notes: null

initialization:
  path_to_ckpt: null
  load_denoiser: True
  load_rew_end_model: True
  load_actor_critic: True

common:
  devices: all
  seed: null
  resume: False

checkpointing:
  save_agent_every: 10  # Save less frequently
  num_to_keep: 5

collection:
  train:
    num_envs: 1
    epsilon: 0.01
    num_steps_total: 50000  # Reduced from 100000
    first_epoch:
      min: 2000  # Reduced from 5000
      max: 5000  # Reduced from 10000
      threshold_rew: 10
    steps_per_epoch: 50  # Reduced from 100
  test:
    num_envs: 1
    num_episodes: 4
    epsilon: 0.0
    num_final_episodes: 50  # Reduced from 100

static_dataset:
  path: ${env.path_data_low_res}
  ignore_sample_weights: True

training:
  should: True
  num_final_epochs: 200  # Reduced from 1500
  cache_in_ram: False
  num_workers_data_loaders: 4
  model_free: False
  compile_wm: False

evaluation:
  should: True
  every: 10  # Evaluate less frequently

# Optimized logging for fast training
logging:
  tensorboard_frequency: 5  # Log every 5 epochs
  image_frequency: 20  # Log images every 20 epochs
  wandb_frequency: 5  # Log to wandb every 5 epochs

denoiser:
  training:
    num_autoregressive_steps: 3  # Reduced from 4
    start_after_epochs: 0
    steps_first_epoch: 50  # Reduced from 100
    steps_per_epoch: 50  # Reduced from 100
    sample_weights: null
    batch_size: 8  # Reduced from 14
    grad_acc_steps: 1  # Default for fast training
    lr_warmup_steps: 50  # Reduced from 100
    max_grad_norm: 10.0

  optimizer:
    lr: 2e-4  # Slightly higher learning rate
    weight_decay: 1e-2
    eps: 1e-8
  
  sigma_distribution:
    _target_: models.diffusion.SigmaDistributionConfig
    loc: -1.2
    scale: 1.2
    sigma_min: 2e-3
    sigma_max: 20

upsampler:
  training:
    num_autoregressive_steps: 1
    start_after_epochs: 50  # Start upsampling later
    steps_first_epoch: 100  # Reduced from 400
    steps_per_epoch: 100  # Reduced from 400
    sample_weights: null
    batch_size: 8  # Reduced from 14
    grad_acc_steps: 1  # Default for fast training
    lr_warmup_steps: 50  # Reduced from 100
    max_grad_norm: 10.0

  optimizer: ${denoiser.optimizer}
  sigma_distribution: ${denoiser.sigma_distribution}
